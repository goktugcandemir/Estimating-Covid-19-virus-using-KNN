{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment1 Report                                             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Theory Questions:\n",
    "\n",
    "### k-Nearest Neighbor Classification\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1) Let k-NN(S) denote the k-Nearest Neighbor classifier on a sample set S, containing\n",
    "samples from 2 classes (positive, negative). \n",
    "<br><b>a) Show that if in both 1-NN(S1) and 1-NN(S2) the label of point x is positive,\n",
    "then in 1-NN(S1 U S2) the label of x is positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/s1.png\" style=\"width:600px;height:600\"/>\n",
    "<img src=\"images/s2.png\" style=\"width:600px;height:600\"/>\n",
    "<img src=\"images/s1Us2.png\" style=\"width:600px;height:600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <br><b>b) Show an example such that in both 3-NN(S1) and 3-NN(S2) the label of x is\n",
    "positive, and in 3-NN(S1 U S2) the label of x is negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/s2s1.png\" style=\"width:600px;height:600\"/>\n",
    "<img src=\"images/s2s2.png\" style=\"width:600px;height:600\"/>\n",
    "<img src=\"images/s1Us22.png\" style=\"width:600px;height:600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2) One of the problems with k-nearest neighbor learning is how to select a value for\n",
    "k. Say you are given the following data set. This is a binary classification task in\n",
    "which the instances are described by two real-valued attributes (* and o denote\n",
    "positive and negative classes, respectively).\n",
    "<br>\n",
    "<br><b>a) What value of k minimizes the training set error for this data set, and what\n",
    "is the resulting training set error? Why is training set error not a reasonable\n",
    "estimate of test set error, especially given this value of k?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> <u> Answer: </u> İf we choose k as 0, points can be their own neighbor. So resulting training set error will be 0. We can not make any prediction of this result because we just determined distance from points itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><b>b) What value of k minimizes the leave-one-out cross-validation error for this\n",
    "data set, and what is the resulting error? Why is cross-validation a better\n",
    "measure of test set performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> <u> Answer: </u> Value of k =5  minimizes the leave-one-out cross-validation error for this data set. For example, the first negative instance which is at (1,5) point, to make a true estimate the nearest 4 neighbors are 2 positive and 2 negative. To minimize error, k must be 5. Cross validation a better measure of test set performance because cross validation enables the use of all train data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><b>c) Why might using too large values k be bad in this dataset? Why might too\n",
    "small values of k also be bad?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> <u> Answer: </u> İf k is too large (like k=13) it cause underfitting and every estimation becomes one class which is the majority class in our DataSet Too small k values may cause overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><b>d) Sketch the 1-nearest neighbor decision boundary for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/cevap.png\" style=\"width:400px;height:400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regresyon "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><b>1) Suppose you are given m=23 training examples with n=5 features (excluding the\n",
    "additional all-ones feature for the bias term, which you should add).\n",
    "Recall that the closed form solution of linear regression is $\\theta$ = (XTX)-1XT y. For\n",
    "the given values of m and n, what are the dimensions of X; y; $\\theta$ in this equation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> <u> Answer: </u> We should add all ones feature for the bias term to the X. So X is (mx n+1) dimensional matrix.So X is a 23x6 matrix. Y is (n+1x1) dimensional matrix. $\\theta$ is (mx1) matrix. So X is (23x6) dimensional matrix , Y is (6x1) dimensional matrix and $\\theta$ (23x1) dimensional matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><b>2) Suppose you have m=50 training examples which are represented with n=200,000\n",
    "dimensional feature vectors. You want to use multivariate linear regression to fit\n",
    "paremeters to $\\theta$ our data. Should you prefer gradient descent or the closed form\n",
    "solution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> <u> Answer: </u> I prefer gradient descent. Because closed-form solutions include inverse operations. (XTX)−1  . to do that you should invert the 200001x200001 matrix. This operation is a very costly operation. So it will be very slow. Therefore  I prefer gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><b>3) Which of the following are valid reasons for using feature scaling?\n",
    "<br> <b> <u> Answer: </u> Only C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>a) It speeds up solving for $\\theta$ using the normal equation.\n",
    "<br> <b> <u> Comment: </u> To speeds up, we should decrease the computational cost but feature scaling is not affect the computational cost for the normal equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>b) It prevents the matrix XTX (used in the normal equation) from being noninvertable\n",
    "(singular/degenerate)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> <u> Comment: </u> No . To solve being non invertable situation , we should reduce number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>c) It speeds up gradient descent by making it require fewer iterations to get to\n",
    "a good solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> <u> Comment: </u> Yes. In this case, feature scaling reduces the number of iteration. So It speeds up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>d) It is necessary to prevent gradient descent from getting stuck in local optima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> <u> Comment: </u> Linear regresion only have global optima. It has not local optima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2: Classification of Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; In this part I used Gabor filter to texture extraction. To improve computational time, I converted the 256x256 pixel picture coming out of the gabor filter to 32x32 pixels. As the pixel of the image decreases, the power to display detail decreases and affects the knn algorithm result. However, I did this to increase the processing speed. <br> K-fold cross validation more expensive than test set but slightly better than test-set. I used the k fold cross validation algorithm to find the optimum k value in the KNN algorithm.Before the k cross validation operation, I shuffled the dataset randomly. I chose k value as 5 for cross validation and used 80% of train data each time and 20% of train data for validation. To find the optimum k value in the knn algorithm, I found the best k value according to the train and validation data determined 5 times as a result of the 5-fold cross validation algorithm and plotted this result.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result of 5 fold cross validation, train and validation data changes in each step. Therefore, the best k value can change for each step. I determined the best k value in each step with 5 fold cross validation and plotted the best 5 k value obtained. As it can be seen in the graph, the value of k = 3 is the most repeated value. Therefore, for the knn algorithm and weighted knn algorithm, I took the optimum k value as 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/k1.png\" style=\"width:350px;height:350\"/>\n",
    "<img src=\"images/k2.png\" style=\"width:350px;height:350\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Distance Metrics</h2><br>I preferred the Euclidean distance function for distance metrics. Largly matrix operations are used in the Euclidean distance function. I used the numpy library to make the code run much faster. Thanks to Numpy, I determined that this function works 4 times faster than iterative calculation. <br> <img src=\"images/euclidean.png\" style=\"width:400px;height:400\"/> <br>To measure the distance between my two images , I took the difference of the two pictures in the Euclidean distance function. I squared the resulting matrix. I added all the elements of the resulting matrix and returned the square root of this sum value as a result. The smaller the distance between the two pictures, the more similar the two pictures are. Therefore, I expect the same classified images to have low distances.\n",
    "<h2>KNN Algorithms</h2><br>For each image in the KNN algorithm test data, I used the Euclidean distance function to find the distance values to the entire train data of the image and determined the smallest k distance. For the predict process, I determined the majority class as the predicted value by looking at the label values of the smallest k distances.In these functions, I used the value of k = 3, which I think will give the optimum result I have determined before for the value of k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Weighted k-NN</h2>\n",
    "<br>\n",
    "I weighted the knn algorithm in the weighted knn algorithm. I weighed the obtained distance by multiplying it by 1 / d.  d represents the distance value obtained by the Euclidean distance algorithm. As in knn, I used the value of k as 3 in the weighted knn algorithm.\n",
    "<h2>Accuracy</h2>\n",
    "<br>\n",
    "To test the success of our prediction, we need the accuracy value. I used a confusion matrix to calculate accuracy. I divided the number of correct predictions in the confusion matrix by the total number of tests and multiplied by 100. This result gave me the value of accuracy.\n",
    "\n",
    "<h2>Testing Train Data with Validation Data</h2>\n",
    "<br>\n",
    "Since we used 5-fold cross validation, I reserved 20% of our data for validation and 80% of it as train data. I tested how well we predicted before seeing the test data. As a result of the test, I reached an average of 89-90 accurucy values. Weighed knn algorithm gave better results than knn algorithm in general. This result is also seen in the figure below.\n",
    "<img src=\"images/Figure_5.png\" style=\"width:400px;height:400\"/>\n",
    "<br>\n",
    "In the testing phase, I will analyze why the weighted KNN algorithm works better. I expect to see similar results in test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misclassification Reasons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is costly to process with a 256x256 matrix. To speed up this process, I reduced it to 32x32 matrix size as I mentioned earlier. The smaller the size of the images, the harder it is to distinguish from each other, so I think this is one of the reasons for the wrong guesses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Especially in covid data and Viral Pneumonia data, it is seen that the white tone is intense. Since we are looking at the difference of rgb values between two pictures with Distance metrics, these values come very close to each other. This estimates some covid data as viral Pneumonia and some viral Pneumonia data as covid and misclassifies them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/COVID (1069).png\" style=\"width:250px;height:250\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, this image is an instance of Covid patient .Since the white color is intense in the picture, it was estimated that this picture belongs to the viral Pneumonia patient and was wrongly predicted.\n",
    "<br>\n",
    "In addition, one of the reasons for the wrong estimation is some data in the dataset. There are some blur or extremely bright images in the dataset. Some images have black margins at the edges. Some weird images are also included in the dataset. Some of these are seen below.\n",
    "<br>\n",
    "<img src=\"images/weird1.png\" style=\"width:250px;height:250\"/>\n",
    "<br>\n",
    "<img src=\"images/weird2.png\" style=\"width:250px;height:250\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Test Result and Analysis</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result of the classification estimation I made using test data, I reached an average accuracy of 88-89. In order to test which of the KNN and weighted knn algorithms give better results, I used the cross validation technique to estimate using both knn and weighted knn algorithms on the same train data and plotted the obtained accuracy values on the graph. I came to the conclusion that the weighted knn algorithm gives similar but slightly better results than the knn algorithm. This is seen in the figures below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/Figure_1.png\" style=\"width:450px;height:450\"/>\n",
    "<img src=\"images/Figure_2.png\" style=\"width:450px;height:450\"/>\n",
    "<img src=\"images/Figure_3.png\" style=\"width:450px;height:450\"/>\n",
    "<img src=\"images/Figure_4.png\" style=\"width:450px;height:450\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   I think the reason why the weighted knn algorithm works better is the following. Since we take the value of K as 3, in the knn algorithm, if at least 2 votes are not given to one of the 3 classes, the majority cannot be achieved. This is not a problem since the distances of the closest neighbors are different in the weighted knn algorithm. However, in this type of situation, there is instability in the knn algorithm.In order not to be unstable and to make an estimate in such cases, I set the order of priority as 1) viral Pneumonia 2) normal 3) covid, because in our dataset, there are 1076 viral Pneumonia, 1073 normal, 960 covid samples. So if one of the closest 3 neighbors of our test data is covid, one is normal and one is viral, we used our prediction in favor of viral phnemia. The topics I explained in the Misclassification Reasons title also affected the test result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
